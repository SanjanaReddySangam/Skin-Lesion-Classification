{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdb2PdU-jggy"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "plt.ion()\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "base_dir = '/content/drive/My Drive/Skin Cancer Detection/tot_images'\n",
        "all_image_path = glob(os.path.join(base_dir, '*.jpg'))\n",
        "\n",
        "if not all_image_path:\n",
        "    raise FileNotFoundError(\"No images found in the specified directory.\")\n",
        "else:\n",
        "    print(f\"Number of images found: {len(all_image_path)}\")\n",
        "\n",
        "imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in all_image_path}\n",
        "lesion_type_dict = {\n",
        "    'nv': 'Melanocytic nevi',\n",
        "    'mel': 'Melanoma',\n",
        "    'bkl': 'Benign keratosis-like lesions',\n",
        "    'bcc': 'Basal cell carcinoma',\n",
        "    'akiec': 'Actinic keratoses',\n",
        "    'vasc': 'Vascular lesions',\n",
        "    'df': 'Dermatofibroma'\n",
        "}\n",
        "\n",
        "metadata_path = os.path.join(base_dir, 'HAM10000_metadata.csv')\n",
        "assert os.path.exists(metadata_path), \"Metadata file not found!\"\n",
        "\n",
        "df_original = pd.read_csv(metadata_path)\n",
        "df_original['path'] = df_original['image_id'].map(imageid_path_dict.get)\n",
        "df_original['cell_type'] = df_original['dx'].map(lesion_type_dict.get)\n",
        "df_original['cell_type_idx'] = pd.Categorical(df_original['cell_type']).codes\n",
        "\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# Visualization\n",
        "if all_image_path:\n",
        "    w, h = 10, 10\n",
        "    fig = plt.figure(figsize=(15, 15))\n",
        "    columns, rows = 3, 2\n",
        "    start, end = 0, len(all_image_path) - 1\n",
        "    ax = []\n",
        "    for i in range(columns * rows):\n",
        "        k = random.randint(start, end)\n",
        "        img = mpimg.imread((all_image_path[k]))\n",
        "        ax.append(fig.add_subplot(rows, columns, i + 1))\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.imshow(img, cmap=\"gray\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No images found in the specified directory.\")\n",
        "\n",
        "normMean = [0.7630358, 0.54564357, 0.5700475]\n",
        "normStd = [0.14092763, 0.15261263, 0.16997081]\n",
        "\n",
        "df_undup = df_original.groupby('lesion_id').count()\n",
        "df_undup = df_undup[df_undup['image_id'] == 1]\n",
        "df_undup.reset_index(inplace=True)\n",
        "\n",
        "def get_duplicates(x):\n",
        "    unique_list = list(df_undup['lesion_id'])\n",
        "    if x in unique_list:\n",
        "        return 'unduplicated'\n",
        "    else:\n",
        "        return 'duplicated'\n",
        "\n",
        "df_original['duplicates'] = df_original['lesion_id']\n",
        "df_original['duplicates'] = df_original['duplicates'].apply(get_duplicates)\n",
        "df_undup = df_original[df_original['duplicates'] == 'unduplicated']\n",
        "\n",
        "y = df_undup['cell_type_idx']\n",
        "_, df_val = train_test_split(df_undup, test_size=0.2, random_state=101, stratify=y)\n",
        "\n",
        "def get_val_rows(x):\n",
        "    val_list = list(df_val['image_id'])\n",
        "    if str(x) in val_list:\n",
        "        return 'val'\n",
        "    else:\n",
        "        return 'train'\n",
        "\n",
        "df_original['train_or_val'] = df_original['image_id']\n",
        "df_original['train_or_val'] = df_original['train_or_val'].apply(get_val_rows)\n",
        "df_train = df_original[df_original['train_or_val'] == 'train']\n",
        "\n",
        "data_aug_rate = [15, 10, 5, 50, 0, 40, 5]\n",
        "augmented_df_list = [df_train]\n",
        "\n",
        "for i in range(7):\n",
        "    if data_aug_rate[i] > 1:\n",
        "        df_to_augment = df_train[df_train['cell_type_idx'] == i]\n",
        "        augmented_df_list.append(pd.concat([df_to_augment] * (data_aug_rate[i] - 1), ignore_index=True))\n",
        "\n",
        "df_train = pd.concat(augmented_df_list, ignore_index=True)\n",
        "df_train = df_train.reset_index(drop=True)\n",
        "df_val = df_val.reset_index(drop=True)\n",
        "\n",
        "df_train_subset = df_train.sample(frac=0.1, random_state=101)\n",
        "df_val_subset = df_val.sample(frac=0.1, random_state=101)\n",
        "\n",
        "class PretrainedResNext(nn.Module):\n",
        "    def __init__(self, num_class=7):\n",
        "        super().__init__()\n",
        "        resNext = models.resnext101_32x8d(pretrained=True)\n",
        "        self.channels = resNext.fc.out_features\n",
        "        for params in resNext.parameters():\n",
        "            params.requires_grad_(False)\n",
        "        self.features = nn.Sequential(*list(resNext.children()))\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc1 = nn.Linear(self.channels, num_class)\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "        out = self.relu(features)\n",
        "        out = nn.functional.adaptive_avg_pool2d(out, (1, 1))\n",
        "        out = out.view(-1, self.channels)\n",
        "        out = self.fc1(out)\n",
        "        out = self.softmax(out)\n",
        "        return out\n",
        "\n",
        "model = PretrainedResNext()\n",
        "model_ft = models.resnext101_32x8d(pretrained=True)\n",
        "model_ft.fc = nn.Linear(in_features=2048, out_features=7)\n",
        "model = model_ft\n",
        "\n",
        "input_size = 224\n",
        "train_transform = transforms.Compose([transforms.Resize((input_size, input_size)),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.RandomVerticalFlip(),\n",
        "                                      transforms.RandomRotation(20),\n",
        "                                      transforms.ColorJitter(brightness=0.1, contrast=0.1, hue=0.1),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(normMean, normStd)])\n",
        "val_transform = transforms.Compose([transforms.Resize((input_size, input_size)),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize(normMean, normStd)])\n",
        "\n",
        "df_train_subset = df_train.sample(frac=0.1, random_state=101).reset_index(drop=True)\n",
        "df_val_subset = df_val.sample(frac=0.1, random_state=101).reset_index(drop=True)\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.df.loc[index, 'path']\n",
        "        X = Image.open(img_path)\n",
        "        y = torch.tensor(int(self.df.loc[index, 'cell_type_idx']))\n",
        "\n",
        "        if self.transform:\n",
        "            X = self.transform(X)\n",
        "        return X, y\n",
        "\n",
        "# Create Custom Dataset and DataLoader with reduced workers and no pin_memory\n",
        "batch_size = 16  # Keep the reduced batch size\n",
        "\n",
        "training_set = CustomDataset(df_train_subset, transform=train_transform)\n",
        "train_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=False)\n",
        "validation_set = CustomDataset(df_val_subset, transform=val_transform)\n",
        "val_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False, num_workers=1, pin_memory=False)\n",
        "\n",
        "# AverageMeter class definition\n",
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "# Initialize lists to store training and validation metrics\n",
        "total_loss_train = []\n",
        "total_acc_train = []\n",
        "total_loss_val = []\n",
        "total_acc_val = []\n",
        "\n",
        "# Train and Validate Functions\n",
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    train_loss = AverageMeter()\n",
        "    train_acc = AverageMeter()\n",
        "    curr_iter = (epoch - 1) * len(train_loader)\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "        images, labels = data\n",
        "        N = images.size(0)\n",
        "        images = Variable(images).to(device)\n",
        "        labels = Variable(labels).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        prediction = outputs.max(1, keepdim=True)[1]\n",
        "        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item() / N)\n",
        "        train_loss.update(loss.item())\n",
        "        curr_iter += 1\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f'[epoch {epoch}], [iter {i + 1} / {len(train_loader)}], [train loss {train_loss.avg:.5f}], [train acc {train_acc.avg:.5f}]')\n",
        "\n",
        "    # Append average loss and accuracy to the lists\n",
        "    total_loss_train.append(train_loss.avg)\n",
        "    total_acc_train.append(train_acc.avg)\n",
        "\n",
        "def validate(val_loader, model, criterion, epoch):\n",
        "    model.eval()\n",
        "    val_loss = AverageMeter()\n",
        "    val_acc = AverageMeter()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(val_loader):\n",
        "            images, labels = data\n",
        "            N = images.size(0)\n",
        "            images = Variable(images).to(device)\n",
        "            labels = Variable(labels).to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            prediction = outputs.max(1, keepdim=True)[1]\n",
        "            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item() / N)\n",
        "            val_loss.update(loss.item())\n",
        "\n",
        "    # Append average loss and accuracy to the lists\n",
        "    total_loss_val.append(val_loss.avg)\n",
        "    total_acc_val.append(val_acc.avg)\n",
        "    print(f'[epoch {epoch}], [val loss {val_loss.avg:.5f}], [val acc {val_acc.avg:.5f}]')\n",
        "    return val_acc.avg\n",
        "\n",
        "# Main Training Loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "epoch_num = 5\n",
        "best_val_acc = 0\n",
        "\n",
        "for epoch in range(1, epoch_num + 1):\n",
        "    train(train_loader, model, criterion, optimizer, epoch)\n",
        "    val_acc = validate(val_loader, model, criterion, epoch)\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        print(f'Best model saved with accuracy: {best_val_acc:.5f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(total_loss_train, label='Training Loss')\n",
        "plt.plot(total_loss_val, label='Validation Loss')\n",
        "plt.title('Training and Validation Loss per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plotting training and validation accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(total_acc_train, label='Training Accuracy')\n",
        "plt.plot(total_acc_val, label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XOuGJUUYjjZI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
